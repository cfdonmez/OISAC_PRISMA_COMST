{
    "nbformat": 4,
    "nbformat_minor": 0,
    "metadata": {
        "colab": {
            "provenance": [],
            "machine_shape": "hm",
            "gpuType": "A100"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3"
        },
        "accelerator": "GPU"
    },
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "# üöÄ O-ISAC Extraction Pipeline v3.0\n",
                "\n",
                "**Optimized PRISMA-compliant data extraction for O-ISAC systematic review**\n",
                "\n",
                "## Features:\n",
                "- ‚úÖ **Resume Support** - Only processes new/changed PDFs\n",
                "- ‚úÖ **v2.0 Schema** - PRISMA Protocol Section 9 aligned\n",
                "- ‚úÖ **GPU Optimized** - Batched visual analysis\n",
                "- ‚úÖ **Async LLM** - Parallel Groq API calls\n",
                "\n",
                "## Requirements:\n",
                "- GPU Runtime (A100 recommended)\n",
                "- GROQ_API_KEY in Colab secrets\n",
                "- Google Drive mounted with PDFs"
            ],
            "metadata": {
                "id": "header"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üì¶ Step 1: Install Dependencies"
            ],
            "metadata": {
                "id": "install_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Install required packages\n",
                "!pip install -q marker-pdf openai nest_asyncio transformers torch pillow pandas\n",
                "\n",
                "# Verify marker installation\n",
                "import shutil\n",
                "if shutil.which('marker_single'):\n",
                "    print('‚úÖ Marker installed successfully')\n",
                "else:\n",
                "    print('‚ö†Ô∏è Marker not in PATH - may need runtime restart')"
            ],
            "metadata": {
                "id": "install"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîó Step 2: Mount Google Drive"
            ],
            "metadata": {
                "id": "mount_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Verify project path\n",
                "import os\n",
                "PROJECT = '/content/drive/MyDrive/AKU_WorkSpace/survey_fdgit/OISAC_PRISMA_COMST'\n",
                "PDF_DIR = os.path.join(PROJECT, 'data/retrieved_docs')\n",
                "\n",
                "if os.path.exists(PDF_DIR):\n",
                "    pdfs = [f for f in os.listdir(PDF_DIR) if f.endswith('.pdf')]\n",
                "    print(f'‚úÖ Found {len(pdfs)} PDFs in retrieved_docs')\n",
                "else:\n",
                "    print('‚ùå PDF directory not found!')"
            ],
            "metadata": {
                "id": "mount"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## ‚öôÔ∏è Step 3: Load Pipeline Module"
            ],
            "metadata": {
                "id": "load_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# Add notebooks folder to path and import pipeline\n",
                "import sys\n",
                "sys.path.insert(0, os.path.join(PROJECT, 'analysis/notebooks'))\n",
                "\n",
                "from extraction_pipeline_v3 import (\n",
                "    Config, CheckpointManager,\n",
                "    phase1_marker_conversion,\n",
                "    phase2_visual_analysis,\n",
                "    phase3_llm_extraction,\n",
                "    run_full_pipeline\n",
                ")\n",
                "\n",
                "# Initialize directories\n",
                "Config.init_dirs()\n",
                "print('‚úÖ Pipeline loaded')\n",
                "print(f'üìÅ Output: {Config.OUTPUT_DIR}')"
            ],
            "metadata": {
                "id": "load"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üîë Step 4: Verify API Key\n",
                "\n",
                "Make sure `GROQ_API_KEY` is set in Colab Secrets (üîë icon in left sidebar)"
            ],
            "metadata": {
                "id": "api_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "from google.colab import userdata\n",
                "try:\n",
                "    key = userdata.get('GROQ_API_KEY')\n",
                "    print(f'‚úÖ GROQ_API_KEY found ({key[:8]}...)')\n",
                "except:\n",
                "    print('‚ùå GROQ_API_KEY not found! Add it to Colab Secrets.')"
            ],
            "metadata": {
                "id": "verify_key"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## üéØ Step 5: Run Pipeline\n",
                "\n",
                "Choose your run mode:\n",
                "- **Test**: Process first 3 papers\n",
                "- **Full**: Process all papers\n",
                "- **Resume**: Continue from checkpoint"
            ],
            "metadata": {
                "id": "run_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# ========================================\n",
                "# üß™ TEST RUN (First 3 papers)\n",
                "# ========================================\n",
                "results = run_full_pipeline(limit=3)\n",
                "print(f'\\nüìä Extracted {len(results)} papers')"
            ],
            "metadata": {
                "id": "test_run"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ========================================\n",
                "# üöÄ FULL RUN (All papers)\n",
                "# ========================================\n",
                "# Uncomment to run all:\n",
                "# results = run_full_pipeline()\n",
                "# print(f'\\nüìä Extracted {len(results)} papers')"
            ],
            "metadata": {
                "id": "full_run"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# ========================================\n",
                "# ‚è≠Ô∏è SKIP PHASES (Resume from checkpoint)\n",
                "# ========================================\n",
                "# If Marker/Visual already done, skip to LLM:\n",
                "# results = run_full_pipeline(skip_phase1=True, skip_phase2=True)"
            ],
            "metadata": {
                "id": "skip_phases"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "## üìä Step 6: View Results"
            ],
            "metadata": {
                "id": "results_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import pandas as pd\n",
                "\n",
                "csv_path = os.path.join(Config.OUTPUT_DIR, 'extraction_v3.csv')\n",
                "if os.path.exists(csv_path):\n",
                "    df = pd.read_csv(csv_path)\n",
                "    print(f'üìä Total experiments: {len(df)}')\n",
                "    print(f'üìÑ Total papers: {df[\"Paper_ID\"].nunique()}')\n",
                "    print('\\n--- Sample Data ---')\n",
                "    display(df.head(10))\n",
                "else:\n",
                "    print('No results yet - run the pipeline first!')"
            ],
            "metadata": {
                "id": "view_results"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Distribution analysis\n",
                "if 'df' in dir() and len(df) > 0:\n",
                "    print('\\nüìà Medium Distribution:')\n",
                "    print(df['Medium'].value_counts())\n",
                "    \n",
                "    print('\\nüìà ISAC Waveform Relationship:')\n",
                "    print(df['ISAC_Relationship'].value_counts())\n",
                "    \n",
                "    print('\\nüìà Coupling Mode:')\n",
                "    print(df['Coupling_Mode'].value_counts())"
            ],
            "metadata": {
                "id": "analysis"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "source": [
                "---\n",
                "## üîß Utility: Check Checkpoint Status"
            ],
            "metadata": {
                "id": "utils_header"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "# View checkpoint status\n",
                "import json\n",
                "checkpoint_path = os.path.join(Config.OUTPUT_DIR, 'checkpoint.json')\n",
                "if os.path.exists(checkpoint_path):\n",
                "    with open(checkpoint_path) as f:\n",
                "        cp = json.load(f)\n",
                "    print(f'‚úÖ Processed: {len(cp.get(\"processed\", {}))} papers')\n",
                "    print(f'‚ùå Errors: {len(cp.get(\"errors\", []))}')\n",
                "    print(f'üïê Last run: {cp.get(\"last_run\", \"Never\")}')\n",
                "else:\n",
                "    print('No checkpoint file yet')"
            ],
            "metadata": {
                "id": "checkpoint"
            },
            "execution_count": null,
            "outputs": []
        },
        {
            "cell_type": "code",
            "source": [
                "# Force reprocess all (clears checkpoint)\n",
                "# Uncomment to reset:\n",
                "# import os\n",
                "# checkpoint_path = os.path.join(Config.OUTPUT_DIR, 'checkpoint.json')\n",
                "# if os.path.exists(checkpoint_path):\n",
                "#     os.remove(checkpoint_path)\n",
                "#     print('üóëÔ∏è Checkpoint cleared!')"
            ],
            "metadata": {
                "id": "clear_checkpoint"
            },
            "execution_count": null,
            "outputs": []
        }
    ]
}