{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# O-ISAC Systematic Review - Search and Deduplication\n",
    "\n",
    "This notebook implements the deduplication pipeline described in the protocol (Section 7.1).\n",
    "\n",
    "**Steps:**\n",
    "1. Load search results from `data/raw_search_results/` (CSV/RIS).\n",
    "2. Normalize titles and authors.\n",
    "3. Identify duplicates using DOI, Title similarity, and Year.\n",
    "4. Merge duplicates, prioritizing peer-reviewed versions.\n",
    "5. Export unique records to `screening/screening_log.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "RAW_DATA_DIR = \"../../data/raw_search_results\"\n",
    "SCREENING_LOG_PATH = \"../../screening/screening_log.csv\"\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    # Lowercase, remove punctuation, remove extra whitespace\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def load_data():\n",
    "    # Placeholder for loading logic. \n",
    "    # In a real scenario, we would parse RIS files or specific CSV exports from IEEE/Scopus.\n",
    "    # For now, we assume standard CSVs with columns: Title, Authors, Year, DOI, Venue, Abstract\n",
    "    all_files = glob.glob(os.path.join(RAW_DATA_DIR, \"*.csv\"))\n",
    "    df_list = []\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            temp_df = pd.read_csv(f)\n",
    "            temp_df['source_file'] = os.path.basename(f)\n",
    "            df_list.append(temp_df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {f}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        print(\"No data found. Creating empty DataFrame for demonstration.\")\n",
    "        return pd.DataFrame(columns=['Title', 'Authors', 'Year', 'DOI', 'Venue', 'Abstract'])\n",
    "        \n",
    "    return pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "def deduplicate(df):\n",
    "    print(f\"Initial records: {len(df)}\")\n",
    "    \n",
    "    # 1. Exact DOI Match\n",
    "    # Normalize DOI\n",
    "    df['norm_doi'] = df['DOI'].astype(str).str.lower().str.strip()\n",
    "    \n",
    "    # 2. Title Match\n",
    "    df['norm_title'] = df['Title'].apply(normalize_text)\n",
    "    \n",
    "    # Create a unique ID for deduplication grouping\n",
    "    # Priority: DOI -> Title+Year\n",
    "    \n",
    "    # Simple deduplication strategy: drop duplicates based on DOI first (if valid), then Title\n",
    "    # Note: This is a simplified version. The protocol suggests a more complex hierarchy.\n",
    "    \n",
    "    # Filter out invalid DOIs for the DOI check\n",
    "    valid_dois = df[df['norm_doi'].str.len() > 5]\n",
    "    invalid_dois = df[df['norm_doi'].str.len() <= 5]\n",
    "    \n",
    "    deduped_dois = valid_dois.drop_duplicates(subset=['norm_doi'], keep='first')\n",
    "    \n",
    "    # For invalid DOIs, deduplicate by Title\n",
    "    deduped_titles = invalid_dois.drop_duplicates(subset=['norm_title'], keep='first')\n",
    "    \n",
    "    # Combine (this is a heuristic, real implementation needs more care to merge info)\n",
    "    final_df = pd.concat([deduped_dois, deduped_titles])\n",
    "    \n",
    "    # Final check on Title for the combined set (in case a DOI record matches a non-DOI record by title)\n",
    "    final_df = final_df.drop_duplicates(subset=['norm_title'], keep='first')\n",
    "    \n",
    "    print(f\"Final unique records: {len(final_df)}\")\n",
    "    return final_df\n",
    "\n",
    "def export_screening_log(df):\n",
    "    # Prepare columns for screening log\n",
    "    log_df = pd.DataFrame()\n",
    "    log_df['record_id'] = range(1, len(df) + 1)\n",
    "    log_df['title'] = df['Title']\n",
    "    log_df['year'] = df['Year']\n",
    "    log_df['source'] = df.get('Venue', 'Unknown')\n",
    "    log_df['status'] = 'unsure' # Default status\n",
    "    log_df['exclusion_reason'] = ''\n",
    "    \n",
    "    # Check if log exists to preserve existing decisions (optional advanced feature)\n",
    "    if os.path.exists(SCREENING_LOG_PATH):\n",
    "        print(\"Screening log exists. Appending new records not implemented in this demo.\")\n",
    "        # In a real run, we would merge and only add new records.\n",
    "    \n",
    "    log_df.to_csv(SCREENING_LOG_PATH, index=False)\n",
    "    print(f\"Exported {len(log_df)} records to {SCREENING_LOG_PATH}\")\n",
    "\n",
    "# Main Execution\n",
    "df = load_data()\n",
    "if not df.empty:\n",
    "    clean_df = deduplicate(df)\n",
    "    export_screening_log(clean_df)\n",
    "else:\n",
    "    print(\"No data to process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}